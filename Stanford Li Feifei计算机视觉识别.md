# Stanford Li Feifei计算机视觉识别

## Lecture1 概述

1. **SIFT特征**、**HOG特征**有被提到，都是经典特征，但它们都是先提取特征再识别，特征提取和识别是两个独立的过程。

2. **过拟合**，指在拟合一个统计模型时，使用过多参数。对比于可获取的数据总量来说，一个荒谬的模型只要足够复杂，是可以完美地适应数据。过拟合一般可以视为违反奥卡姆剃刀原则。

   当可选择的参数的自由度超过数据所包含信息内容时，这会导致最后（拟合后）模型使用任意的参数，这会减少或破坏模型一般化的能力更甚于适应数据。过拟合的可能性不只取决于参数个数和数据，也跟模型架构与数据的一致性有关。此外对比于数据中预期的噪声或错误数量，跟模型错误的数量也有关。

3. **ImageNet**，一个数据集合，拥有大量图像数据。

4. **卷积神经网络**（Convolutional Neural Network, **CNN**）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。

   卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。

5. 1998年即产生卷积神经网络用于识别手写字体，这是卷积神经网络的最早的应用。之所以没有以更大的作用出现在人们生活中是因为以下原因
   1. 计算力不足，直到近些年因为计算机的高速发展，才产生了满足用于大量图片识别的卷积神经网络的计算机；
   2. 图像及相应标签不足，上个世纪九十年代互联网还不如现在发达，图像素材匮乏，标签信息更少，因此没有充足的训练样本用于卷积神经网络的训练；

## Lecture2

#### 图像分类数据驱动方法

1. **语义鸿沟**，语义鸿沟是指通常人们在判别图像的相似性时并非建立在图像低层视觉特征的相似上，而是建立在对图像所描述的对象或事件的语义理解的基础上。

   目前，基于内容的图像检索CBIR ( Content-based image retrieval)系统中广泛存在的“语义鸿沟”问题。
   这种理解无法从图像的视觉特征直接获得，它需要使用人们日常生活中积累的大量经验和知识来进行推理和判断。其中,尤其对于一些高层次的抽象概念，如一幅关于节日的图像所表达出的欢乐和喜庆的感觉等，更需要根据人的知识来判断。换言之，人们是依据图像的语义信息来进行图像相似性判别的。正是由于人对图像相似性的判别依据与计算机对相似性的判别依据之间的不同，造成了人所理解的“语义相似”与计算机理解的“视觉相似”之间的“语义鸿沟”的产生。

2. 有两种分类驱动驱动方式，一种是分类规则驱动，一种是数据驱动方法。分类规则驱动需要写好区分各类目标的规则，然后进行图像分类，好处是只需要能归纳出规则的最少训练素材，结构简单，但规则编写过程复杂；数据驱动方法则不需要亲自编写分类规则，但需要大量所需目标的训练样本送入训练机器，由机器自动归纳各类目标特征，且训练机器编写较困难，但最终识别率较高。

   数据驱动的图像分类方法内大致由两个函数构成。一个是训练函数，这个函数接收图片和标签，然后输出模型；另一种是预测函数，接收一个模型，对图片种类进行预测。

#### K近邻分类器

1. **超参数**，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。

   K近邻分类器中**超参数K**的选择策略错误示范：

   1. K = 1，这在训练时往往能达到很好的效果，但是在分类尚未训练过的目标时，将会出现错误，这会导致很差的可拓展性；
   2. K选择在完成训练后的测试中表现最好的数值。这时的K仅能代表在这一组测试集中的最优超参数，不能扩展为“所有未知数据的最优K”，不要这么做。

   正确做法是将数据分为三组，训练集、验证集和测试集，在训练集中挑选出表现最优的K进入验证集并验证是否与训练集效果一致，若效果一致且良好，再进入测试集最终证明效果的优劣，并得出结论。这种方法才能评价该算法在未见的新数据上表现如何。要注意的是，测试集和先前的训练集还有验证集要严格分离，确保测试没有受到污染，这样才能得到没有偏差的数据。 

   还有一种不常用的方法称为“交叉验证”，和上文原理基本相同，但区别在于将除了测试集以外的数据分为若干组，比如$n$组，其中$n-1$组用于训练，1组用于验证，训练完毕后可以用不同的一组数据进行验证，即用不同的$n-1$组进行训练，但最后的测试组都是一致的，这种叫“交叉验证”，但由于深度学习本身已经消耗了大量运算，这一方法又将训练计算量成倍增长，因此不常用。

2. K近邻的局限性：

   1. 当你将同一张图平移、遮挡或者调色后，求他们的和原图的曼哈顿距离，实际上会非常相似，以至于在几何距离这个特征上，几乎无法将这些变化分离开来，不利于视觉变化的区分。
   2. 维数灾难：我们都希望训练数据能较为均匀地分布在训练空间中，这样某一测试数据的输入才不会出现和训练样本的几何距离过远，几何距离过远意味着二者相似度不高，这也就降低了分类的正确率，因此我们需要训练数据能较为均匀地分布在训练空间中，但这会使训练数据的量随着几何特征的维度成指数倍的增加，假如一维几何距离特征向量需要4个训练样本即可较好将训练空间等间距分开，那么二维几何距离特征向量就可能需要16个训练样本，三维需要64个，以此类推。我们根本不可能找到如此多的图像去均匀填充训练空间。

#### 线性分类器

1. 每个分类**一个**权重模板，对向量化的图像进行矩阵乘法（包括但不限于乘法运算）计算出最终的权重值，并根据最终的权重值进行分类
2. 线性分类器存在无法有效分辨在空间中变换的物体，如平移、旋转、翻转后，便不能有效识别。

## Lecture3

#### 线性分类器权重分配

##### 多分类SVM损失函数

损失函数就是用来衡量一个预测器在对输入数据进行分类预测时的质量好坏。

1. 多分类SVM损失函数
   $$
   L_i=\sum_{j \neq y_i}
   		\begin{equation} \left\{ \begin{array}{**lr**}
   		0 & if \ \  s_{y_i} \geq s_j +1 \\
   		s_j - s_{y_i} + 1 & otherwise
   		\end{array}
   \right.
   \end{equation}\\
   =\sum_{j \neq y_i}max(0,\ s_j-s_{y_i}+1)\ \ \ \ \ \ \ \ \ \ \ \ \ \\
   L = \frac 1 N \sum _{i=1}^N L_i
   $$
   

其中$s_j$是非目标分类的线性分类器得分，$s_{y_i}$是当前目标分类的线性分类器得分。不一定需要求平均值，因为相对大小关系不会因为求平均而改变。

2. 使$L$为0的W不唯一

3. 可以在$L$后加入正则量，使过拟合或更复杂的规则受到惩罚，使简单规则获得优势。可以使用L2范数（2范数）即：
   $$
   L_i = \sum _{j \neq y_i}max(0,\ s_j-s_{y_i}+1)+\lambda R(W)
   $$
   

   其中$R(W)$就是L2范数。也可以使用L1范数，根据问题的具体情况做决定，不要贸然选择。

##### 多项逻辑斯蒂回归（softmax loss）

$$
P(Y = k|X = x_i)=\frac{e^sk}{\sum_je^{s_j}}
$$

其中$s=f(x_i;W)$，即目标得分

则
$$
L_i = -log_{10}P(Y=y_i|X=x_i)
$$
即
$$
L_i=-log_{10}(\frac {e^{s_{y_i}}}{\sum _j e^{s_j}})
$$
越小越好，最小值可能值是0，最大可能值是正无穷。

#### 权重优化

假如权重数量不多，可以使用有限差分法来计算数值梯度，进而优化权重，但不建议这么做，因为效率很低。

计算解析梯度是一种更好的方法，向损失函数的梯度方向的反方向不断前进，前进步长是一个超参数，前进步长也可以叫做学习率，很重要，需要更优先地设置好。

## Lecture4

#### 计算任意复杂函数的解析梯度（反向传播）

利用反向传播算法。先自左至右建立函数的正向计算图，为每个计算节点设置变量名。然后自右至左翻向计算各节点变量关于其右侧节点变量的偏导数。 填充计算图中每个节点关于上一级节点的偏导数值，利用链式法则即可自由计算各局部梯度。

当计算“Max门“的梯度时，取到最大值的变量的偏导数值（梯度值）为上一级的偏导数值，其余取0。

关于向量和矩阵的反向传播，详见P9 46:00。

#### 神经网络简述

上文中的单层线性分类函数：
$$
f = Wx
$$
现在即将要讲的2层神经网络：
$$
f = W_2max(0,W_1x)
$$
2层神经网络即在单层线性分类函数的基础上，加了一层线性分类函数，实现非线性。

同理，3层神经网络：
$$
f=W_3max(0,W_2max(0,W_1x))
$$
激活函数

## Lecture 5

#### 卷积神经网络

于神经网络类似，区别在于需要加入训练卷积层，因为卷积层更能保留输入的空间结构。

#### 卷积神经网络历史简述

1. 50年代，Hubel和Wiesel做了一系列实验，利用猫作为实验对象，研究了图像信息如何作用于脑中神经元。发现在输入不同图像信息后，脑中的一部分会兴奋。同时，视网膜上，不同的细胞会对不同方向的线条、不同的颜色敏感。
2. 1980年，一种被称为“神经认知机”的网络框架模型出现了，实现了上一条中的不同细胞的分工结构，即简单细胞和复杂细胞的交替层结构。对“简单细胞”可以调参，“复杂细胞”则在“简单细胞”之上，进行一种采样（汇合）操作。
3. 1988年，首次出现了应用反向传播和基于梯度的学习方法来训练卷积神经网络，并可应用于手写字母识别，尤其是邮政编码的识别。然而扩展性有限，无法用于更复杂的数据。
4. 2012年，提出了一种现代化的卷积神经网络，相比于上条中的卷积神经网络，新卷积神经网络更大更深，可以充分利用大量数据。

#### 卷积和池化

使用一个比原图像小很多，但通道数相同的卷积和与原图像做卷积运算。设卷积核为$W$，当前点积值为$W^Tx+b$，其中$b$为偏置项。

卷积后得到的激活映射维度会发生改变，假如对个32x32x3的图片使用5x5x3的卷积核逐个像素滑动卷积，得到的激活映射会是28x28x1。原因下文中会讲。

可以在同一张（组）图片中依次使用多个不同的卷积核以获得更多的模式信息，即假如使用6个卷积核，便可得到一个6层的激活映射。

激活映射输出尺寸计算：$(N-F)/d+1$，$N$是图像边长，一般认为是正方形的图像；$F$是卷积核变长，同样也是正方形；$d$是滑动步长。这就解释了上文中“32x32x3的图片使用5x5x3的卷积核逐个像素滑动卷积，得到的激活映射会是28x28x1”的原因。为了解决激活映射越来越小的问题，需要在原始图像外围填0，计算方式与上公式相同。

#### 池化

图像中的相邻像素倾向于具有相似的值，因此通常卷积层相邻的输出像素也具有相似的值。这意味着，卷积层输出中包含的大部分信息都是冗余的。

如果使用边缘检测滤波器并在某个位置找到强边缘，那么我们也可能会在距离这个像素1个偏移的位置找到相对较强的边缘。但是它们都一样是边缘，我们并没有找到任何新东西。

池化层解决了这个问题。这个网络层所做的就是通过减小输入的大小降低输出值的数量。

池化一般通过简单的最大值、最小值或平均值操作完成。

一般不在池化层填零，池化层只用于降采样。

## Lecture 6 训练神经网络（上）

#### 激活函数

激活函数是一系列非线性函数，用于将上一层神经元的输出在值域内分值并加入非线性性。假如没有激活函数，先前的正向传播的损失函数都是线性的，根据线性函数具有可加性和齐次性的特点，多个线性函数链接可以由由单一线性函数表示，即多层神经元链接可由一层神经元代替，这就有悖于设置多层神经元希望计算更深层特点的初衷；其次，因为线性函数具有线性性，所以不能做到非线性分类；同时，输入值域可能会很大，导致损失函数的值域也会非常大，不利于权重的更新，所以需要限制输入和损失函数的值域；此外，假如神经网络函数是线性的，那么其导数将会是一个常数，与输入无关，反向传播时，梯度值也与输入无关了。综上，需要一个非线性，值域有限的函数链接在神经元上，解决以上问题。

#### 常见激活函数

##### Sigmoid函数

$$
\sigma(x)=1/(1+e^{-x})
$$

每个元素会被压缩到$[0,1]$的值域内。

缺点：

1. 饱和神经元将使梯度消失：输入过大或过小时，梯度为0，继续反向传播时，会使上游梯度也变为0；
2. 非零中心函数：使下级函数值全部为正或负，不利于权重更新；
3. 指数函数计算代价较高（不重要的缺点）

##### tanh(x)函数（双曲正切函数）

$$
f(x)=tanh(x)
$$



每个元素会被压缩到$[-1,1]$的值域内。

缺点：饱和神经元将使梯度消失。

##### ReLU函数

$$
f(x)=max(0,x)
$$

输入负数，输出为0；输入正数，输出不变。在正数范围不会饱和，可以计算的更快。

缺点：

1. 非零中心函数；
2. 负半轴梯度消失。dead ReLU

##### Leaky ReLU函数

$$
f(x)=max(0.01x, x)
$$

负半轴存在较小斜率，解决了ReLU函数负半轴饱和的问题。

##### 经验

1. 一般情况用ReLU，小心设置学习率
2. 可以试试Leaky ReLU
3. 也可以试试tanh，但别抱太大希望
4. 别用sigmoid函数

#### 批量归一化

通道零均值化，图片各通道减去各通道对应的所有样本的均值。

#### 初始化权重

初始权重过大，网络饱和；初始权重过小，网络崩溃

初始方法是：使初始输入权重的方差，等于输出的方差。注意ReLU激活函数会使一般神经元置零的情况。


